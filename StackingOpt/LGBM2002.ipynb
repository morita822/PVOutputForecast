{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG3:\n",
    "    DEBUG = True\n",
    "    note_num = \"StackingOpt/LGBM003/CFG3\"\n",
    "    load_data_kind = \"load_data2\"\n",
    "    n_splits = 5  #データの分割,クロスバリデーション（機械学習、学習データの分割）\n",
    "    seed = 42\n",
    "    \n",
    "    #model\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'num_boost_round': 3,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 5,\n",
    "        'device_type': 'cpu',  # ここを 'cpu' に変更\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    \n",
    "    #日付\n",
    "    train_start_date = \"201406010000\"\n",
    "    train_end_date = \"201407010000\"\n",
    "    test_start_date = \"201407010000\"\n",
    "    test_end_date = \"201408010000\"\n",
    "\n",
    "    #特徴量\n",
    "    flo_unique_dir = 'H:/study/output/StackingOpt/EDA006/'\n",
    "\n",
    "    features = ['two_weeks_max', 'id', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'year_sin', 'year_cos',\\\n",
    "                'prev_30m_generation', 'id_lat', 'id_lng', 'id_lat_mesh', 'id_lng_mesh', 'pvrate', 'observed_max2']\n",
    "\n",
    "\n",
    "    use_flo_unique_features = False #オプティカルフローのunique_idの予測値\n",
    "\n",
    "    target = 'nv2'\n",
    "\n",
    "    #oofで保存するcol\n",
    "    saved_cols = [\"datetime\",\"id\",\"fold\",\"observed_max\",\"generation\",target,\"pred\"]\n",
    "\n",
    "cfg = CFG3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tus73\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000070\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000272 seconds, init for row-wise cost 0.009035 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2463\n",
      "[LightGBM] [Info] Number of data points in the train set: 215280, number of used features: 15\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 0.420561\n",
      "[LightGBM] [Debug] Re-bagging, using 172076 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.230031\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.221126\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[3]\tvalid_0's l1: 0.212702\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's l1: 0.212702\n",
      "Fold 1 MSE: 0.05883734089686842, MAE: 0.21270177389938436\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tus73\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.002860\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000003 seconds, init for row-wise cost 0.008441 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2461\n",
      "[LightGBM] [Info] Number of data points in the train set: 215280, number of used features: 15\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 0.423102\n",
      "[LightGBM] [Debug] Re-bagging, using 172076 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.212633\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.204409\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[3]\tvalid_0's l1: 0.196688\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's l1: 0.196688\n",
      "Fold 2 MSE: 0.05168528361098788, MAE: 0.19668764596802835\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tus73\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.002872\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000005 seconds, init for row-wise cost 0.008165 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2461\n",
      "[LightGBM] [Info] Number of data points in the train set: 215280, number of used features: 15\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 0.410337\n",
      "[LightGBM] [Debug] Re-bagging, using 172076 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.214725\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.206205\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[3]\tvalid_0's l1: 0.198258\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's l1: 0.198258\n",
      "Fold 3 MSE: 0.054204656592230636, MAE: 0.19825771481408777\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tus73\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.002876\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000004 seconds, init for row-wise cost 0.008830 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2461\n",
      "[LightGBM] [Info] Number of data points in the train set: 215280, number of used features: 15\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 0.405722\n",
      "[LightGBM] [Debug] Re-bagging, using 172076 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.227515\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.218433\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[3]\tvalid_0's l1: 0.209887\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's l1: 0.209887\n",
      "Fold 4 MSE: 0.05904309535713117, MAE: 0.2098867644804626\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tus73\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.002855\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000003 seconds, init for row-wise cost 0.008124 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2461\n",
      "[LightGBM] [Info] Number of data points in the train set: 215280, number of used features: 15\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score 0.413141\n",
      "[LightGBM] [Debug] Re-bagging, using 172076 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.216335\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.207815\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[3]\tvalid_0's l1: 0.199836\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3]\tvalid_0's l1: 0.199836\n",
      "Fold 5 MSE: 0.05395766563986323, MAE: 0.19983614356363713\n",
      "----------------------------------------\n",
      "Overall Out-of-Fold RMSE: 0.2357\n",
      "Overall Out-of-Fold MAE: 0.2035\n",
      "\n",
      "Overall Test RMSE: 0.2338\n",
      "Overall Test MAE: 0.2039\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG:\n",
    "    OUTPUT_DIR = f'E:/study/output/DEBUG/{cfg.note_num}/'\n",
    "else:\n",
    "    OUTPUT_DIR = f'E:/study/output/{cfg.note_num}/'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "utils.set_seeds()\n",
    "# 時系列の分割設定\n",
    "train_date_list = utils.create_time_series_data(cfg.train_start_date,cfg.train_end_date)\n",
    "train_date_list_split = np.array_split(train_date_list, cfg.n_splits)\n",
    "\n",
    "test_dates = utils.create_time_series_data(cfg.test_start_date,cfg.test_end_date)\n",
    "\n",
    "#データセットの読み込み\n",
    "if cfg.load_data_kind == \"load_data1\":\n",
    "    df,unique_id = utils.load_data1()\n",
    "elif cfg.load_data_kind == \"load_data2\":\n",
    "    df,unique_id = utils.load_data2()\n",
    "\n",
    "#oof作成用\n",
    "df[\"pred\"] = 0\n",
    "df.loc[df.datetime.isin(test_dates),\"fold\"] = \"test\"\n",
    "\n",
    "for fold in range(len(train_date_list_split)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "    train_dates = np.concatenate(train_date_list_split[:fold] + train_date_list_split[fold+1:])\n",
    "    valid_dates = train_date_list_split[fold]\n",
    "\n",
    "    X_train, y_train = df.loc[df.datetime.isin(train_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(train_dates),cfg.target]\n",
    "    X_valid, y_valid = df.loc[df.datetime.isin(valid_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(valid_dates),cfg.target]\n",
    "    X_test, y_test = df.loc[df.datetime.isin(test_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(test_dates),cfg.target]\n",
    "    df.loc[df.datetime.isin(valid_dates),\"fold\"] = fold\n",
    "\n",
    "    if cfg.use_flo_unique_features:\n",
    "        X_train_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,train_dates,unique_id)\n",
    "        X_valid_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,valid_dates,unique_id)\n",
    "        X_test_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,test_dates,unique_id)\n",
    "        \n",
    "        X_train_flo_unique = pd.DataFrame(X_train_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_train_flo_unique.shape[1])])\n",
    "        X_train_flo_unique[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "        X_valid_flo_unique= pd.DataFrame(X_valid_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_valid_flo_unique.shape[1])])\n",
    "        X_valid_flo_unique[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "        X_test_flo_unique= pd.DataFrame(X_test_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_test_flo_unique.shape[1])])\n",
    "        X_test_flo_unique[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "        X_train = X_train.merge(X_train_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "        X_valid = X_valid.merge(X_valid_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "        X_test = X_test.merge(X_test_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "\n",
    "    X_train.drop(\"datetime\",axis=1,inplace=True)\n",
    "    X_valid.drop(\"datetime\",axis=1,inplace=True)\n",
    "    X_test.drop(\"datetime\",axis=1,inplace=True)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    model = utils.train_lgbm(X_train, y_train, X_valid, y_valid, cfg.lgb_params)\n",
    "    save_path = OUTPUT_DIR + f\"/lgbm_fold{fold}.txt\"\n",
    "    model.save_model(save_path)\n",
    "\n",
    "    # Evaluate model\n",
    "    valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    mse = utils.compute_mse(y_valid, valid_preds)\n",
    "    mae = utils.compute_mae(y_valid, valid_preds)\n",
    "    print(f\"Fold {fold + 1} MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    df.loc[df.datetime.isin(valid_dates),\"pred\"] = valid_preds\n",
    "    df.loc[df.datetime.isin(test_dates),\"pred\"] += test_preds\n",
    "\n",
    "df.loc[df.datetime.isin(test_dates),\"pred\"] /= len(train_date_list_split)\n",
    "df.loc[df.datetime.isin(train_date_list+test_dates),cfg.saved_cols].to_csv(OUTPUT_DIR+\"oof.csv\",index=False)\n",
    "\n",
    "oof_mse = utils.compute_mse(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "test_mse = utils.compute_mse(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "oof_mae = utils.compute_mae(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "test_mae = utils.compute_mae(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "print('-'*40)\n",
    "print(f\"Overall Out-of-Fold RMSE: {np.sqrt(oof_mse):.4f}\")\n",
    "print(f\"Overall Out-of-Fold MAE: {oof_mae:.4f}\")\n",
    "print()\n",
    "print(f\"Overall Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "print(f\"Overall Test MAE: {test_mae:.4f}\")\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.read_csv(OUTPUT_DIR+\"oof.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>observed_max</th>\n",
       "      <th>generation</th>\n",
       "      <th>nv2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-06-01 07:00:00</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>3071.7414</td>\n",
       "      <td>0.594653</td>\n",
       "      <td>0.459352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-06-01 07:30:00</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>3577.7210</td>\n",
       "      <td>0.692605</td>\n",
       "      <td>0.459352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-06-01 08:00:00</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>4000.3258</td>\n",
       "      <td>0.774416</td>\n",
       "      <td>0.459352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-06-01 08:30:00</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>4299.9360</td>\n",
       "      <td>0.832418</td>\n",
       "      <td>0.459352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-06-01 09:00:00</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>4500.6054</td>\n",
       "      <td>0.871265</td>\n",
       "      <td>0.459352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime     id fold  observed_max  generation       nv2  \\\n",
       "0  2014-06-01 07:00:00  564.0    0     5150.5454   3071.7414  0.594653   \n",
       "1  2014-06-01 07:30:00  564.0    0     5150.5454   3577.7210  0.692605   \n",
       "2  2014-06-01 08:00:00  564.0    0     5150.5454   4000.3258  0.774416   \n",
       "3  2014-06-01 08:30:00  564.0    0     5150.5454   4299.9360  0.832418   \n",
       "4  2014-06-01 09:00:00  564.0    0     5150.5454   4500.6054  0.871265   \n",
       "\n",
       "       pred  \n",
       "0  0.459352  \n",
       "1  0.459352  \n",
       "2  0.459352  \n",
       "3  0.459352  \n",
       "4  0.459352  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>observed_max</th>\n",
       "      <th>generation</th>\n",
       "      <th>nv2</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>test</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>1845.2872</td>\n",
       "      <td>0.357226</td>\n",
       "      <td>0.396203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>2014-07-01 07:30:00</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>test</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>2830.1376</td>\n",
       "      <td>0.547882</td>\n",
       "      <td>0.417653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>2014-07-01 08:00:00</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>test</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>1342.0332</td>\n",
       "      <td>0.259802</td>\n",
       "      <td>0.449809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>2014-07-01 08:30:00</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>test</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>2931.8792</td>\n",
       "      <td>0.567578</td>\n",
       "      <td>0.408370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>2014-07-01 09:00:00</td>\n",
       "      <td>5.640000e+02</td>\n",
       "      <td>test</td>\n",
       "      <td>5150.5454</td>\n",
       "      <td>3960.7984</td>\n",
       "      <td>0.766764</td>\n",
       "      <td>0.453815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547165</th>\n",
       "      <td>2014-07-31 16:00:00</td>\n",
       "      <td>6.950000e+09</td>\n",
       "      <td>test</td>\n",
       "      <td>18880.4616</td>\n",
       "      <td>3977.2572</td>\n",
       "      <td>0.205877</td>\n",
       "      <td>0.409674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547166</th>\n",
       "      <td>2014-07-31 16:30:00</td>\n",
       "      <td>6.950000e+09</td>\n",
       "      <td>test</td>\n",
       "      <td>18880.4616</td>\n",
       "      <td>5679.7408</td>\n",
       "      <td>0.294004</td>\n",
       "      <td>0.383995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547167</th>\n",
       "      <td>2014-07-31 17:00:00</td>\n",
       "      <td>6.950000e+09</td>\n",
       "      <td>test</td>\n",
       "      <td>18880.4616</td>\n",
       "      <td>4034.7046</td>\n",
       "      <td>0.208851</td>\n",
       "      <td>0.403061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547168</th>\n",
       "      <td>2014-07-31 17:30:00</td>\n",
       "      <td>6.950000e+09</td>\n",
       "      <td>test</td>\n",
       "      <td>18880.4616</td>\n",
       "      <td>2863.2240</td>\n",
       "      <td>0.148211</td>\n",
       "      <td>0.377709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547169</th>\n",
       "      <td>2014-07-31 18:00:00</td>\n",
       "      <td>6.950000e+09</td>\n",
       "      <td>test</td>\n",
       "      <td>18880.4616</td>\n",
       "      <td>746.2422</td>\n",
       "      <td>0.038628</td>\n",
       "      <td>0.376920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278070 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime            id  fold  observed_max  generation  \\\n",
       "690     2014-07-01 07:00:00  5.640000e+02  test     5150.5454   1845.2872   \n",
       "691     2014-07-01 07:30:00  5.640000e+02  test     5150.5454   2830.1376   \n",
       "692     2014-07-01 08:00:00  5.640000e+02  test     5150.5454   1342.0332   \n",
       "693     2014-07-01 08:30:00  5.640000e+02  test     5150.5454   2931.8792   \n",
       "694     2014-07-01 09:00:00  5.640000e+02  test     5150.5454   3960.7984   \n",
       "...                     ...           ...   ...           ...         ...   \n",
       "547165  2014-07-31 16:00:00  6.950000e+09  test    18880.4616   3977.2572   \n",
       "547166  2014-07-31 16:30:00  6.950000e+09  test    18880.4616   5679.7408   \n",
       "547167  2014-07-31 17:00:00  6.950000e+09  test    18880.4616   4034.7046   \n",
       "547168  2014-07-31 17:30:00  6.950000e+09  test    18880.4616   2863.2240   \n",
       "547169  2014-07-31 18:00:00  6.950000e+09  test    18880.4616    746.2422   \n",
       "\n",
       "             nv2      pred  \n",
       "690     0.357226  0.396203  \n",
       "691     0.547882  0.417653  \n",
       "692     0.259802  0.449809  \n",
       "693     0.567578  0.408370  \n",
       "694     0.766764  0.453815  \n",
       "...          ...       ...  \n",
       "547165  0.205877  0.409674  \n",
       "547166  0.294004  0.383995  \n",
       "547167  0.208851  0.403061  \n",
       "547168  0.148211  0.377709  \n",
       "547169  0.038628  0.376920  \n",
       "\n",
       "[278070 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof[oof.fold==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

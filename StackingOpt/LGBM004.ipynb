{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "import utils\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "\n",
    "class CFG6:\n",
    "    DEBUG = True\n",
    "    note_num = \"StackingOpt/LGBM003/CFG6\"\n",
    "    n_splits = 2  #データの分割\n",
    "    seed = 42\n",
    "    \n",
    "    #model\n",
    "    lgb_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mae',\n",
    "                'num_boost_round':10,\n",
    "                'early_stopping_rounds':100,\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': 5,\n",
    "                'device_type': 'gpu',\n",
    "                'seed':42\n",
    "                }\n",
    "    \n",
    "    #日付\n",
    "    train_start_date = \"201406010000\"\n",
    "    train_end_date = \"201406070000\"\n",
    "    test_start_date = \"201406070000\"\n",
    "    test_end_date = \"201406080000\"\n",
    "\n",
    "    #特徴量\n",
    "    interpolated_dir = \"H:\\study\\output\\StackingOpt\\EDA005\"\n",
    "    pred_dir = \"H:\\study\\output\\StackingOpt\\EDA006\"\n",
    "    flo_dir = 'H:\\study\\output\\StackingOpt\\EDA006'\n",
    "    flo_unique_dir = 'H:/study/output/StackingOpt/EDA006/'\n",
    "\n",
    "    features = ['two_weeks_max', 'id', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'year_sin', 'year_cos',\\\n",
    "                'prev_30m_generation', 'id_lat', 'id_lng', 'id_lat_mesh', 'id_lng_mesh', 'pvrate', 'observed_max']\n",
    "\n",
    "    use_interpolated_features = True\n",
    "    use_pred_features = True\n",
    "    use_flo_features = True\n",
    "    use_flo_lat_features = True\n",
    "    use_flo_lon_features = True\n",
    "    use_flo_unique_features = True #オプティカルフローのunique_idの予測値\n",
    "\n",
    "    target = 'nv2'\n",
    "\n",
    "    #oofで保存するcol\n",
    "    saved_cols = [\"datetime\",\"id\",\"fold\",\"observed_max\",\"generation\",target,\"pred\"]\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "    \n",
    "    if cfg.DEBUG:\n",
    "        OUTPUT_DIR = f'H:/study/output/DEBUG/{cfg.note_num}/'\n",
    "    else:\n",
    "        OUTPUT_DIR = f'H:/study/output/{cfg.note_num}/'\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    utils.set_seeds()\n",
    "    # 時系列の分割設定\n",
    "    train_date_list = utils.create_time_series_data(cfg.train_start_date,cfg.train_end_date)\n",
    "    train_date_list_split = np.array_split(train_date_list, cfg.n_splits)\n",
    "\n",
    "    test_dates = utils.create_time_series_data(cfg.test_start_date,cfg.test_end_date)\n",
    "\n",
    "    #データセットの読み込み\n",
    "    unique_id = ['10000095', '10000269', '1020000002', '1110000001', '1110000010', '1110000011', '1110000012', '1110000013', '1110000014', '1110000015', '1160000025', '1160000090', '1160000091', '1160000182', '1160000185', '1160000253', '1160000387', '1160000402', '1160000419', '1160000420', '1160000423', '1270000026', '1280000048', '1550000001', '1650000004', '1680000001', '1680000002', '1680000003', '1680000004', '1680000010', '1680000017', '1680000021', '1680000033', '1680000047', '1680000054', '1680000057', '1680000063', '1680000067', '1680000080', '1680000081', '1680000097', '1680000107', '1680000108', '1680000112', '1680000151', '1680000152', '1680000213', '1680000216', '1680000217', '1680000218', '1680000223', '1680000228', '1680000285', '1680000287', '1680000327', '1680000364', '2220000001', '2220000002', '2220000003', '2730000001', '2910000002', '3000000007', '3000000012', '3000000042', '5000000044', '5000000045', '6000000016', '6000000017', '6060000016', '6060000017', '6060000018', '6170000016', '6170000123', '6170000124', '6170000125', '6620000065', '6620000066', '6620000088', '6620000089', '6620000111', '6620000117', '6620000118', '6620000121', '6620000122', '6620000123', '6620000124', '6620000131', '6620000132', '6910000180', '6910000198', '6910000200', '6910000206', '6910000216', '6910000217', '6910000239', '6910000240', '6910000249', '6910000250', '6910000421', '6910000424', '6910000425', '6910000469', '6910000470']\n",
    "    unique_id = [int(i) for i in unique_id]\n",
    "    to_unique_id = [str(num).zfill(10) for num in unique_id]\n",
    "    df = utils.get_preprocessing_data(to_unique_id)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.drop_duplicates(subset=[\"id\",\"datetime\"],inplace=True)\n",
    "    df = df.groupby('id').apply(utils.prev_30m_generation).reset_index(level=0, drop=True)\n",
    "    id_all_data = pd.read_csv(\"H:\\study\\output\\StackingOpt\\EDA006\\id_all_data.csv\")\n",
    "    df = df.merge(id_all_data,on=[\"id\"],how=\"left\")\n",
    "    df.dropna(subset=[\"year\"],inplace=True) #utils.prev_30m_generationで30分間隔のデータセットになっているため欠損が出ている。\n",
    "    df[\"nv2\"] = df[\"generation\"] / df[\"observed_max2\"]\n",
    "\n",
    "    #oof作成用\n",
    "    df[\"pred\"] = 0\n",
    "    df.loc[df.datetime.isin(test_dates),\"fold\"] = \"test\"\n",
    "\n",
    "    for fold in range(len(train_date_list_split)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        train_dates = np.concatenate(train_date_list_split[:fold] + train_date_list_split[fold+1:])\n",
    "        valid_dates = train_date_list_split[fold]\n",
    "\n",
    "        X_train, y_train = df.loc[df.datetime.isin(train_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(train_dates),cfg.target]\n",
    "        X_valid, y_valid = df.loc[df.datetime.isin(valid_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(valid_dates),cfg.target]\n",
    "        X_test, y_test = df.loc[df.datetime.isin(test_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(test_dates),cfg.target]\n",
    "        df.loc[df.datetime.isin(valid_dates),\"fold\"] = fold\n",
    "\n",
    "        if cfg.use_interpolated_features:\n",
    "            X_train_interpolated, y_train_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,train_dates)\n",
    "            X_valid_interpolated, y_valid_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,valid_dates)\n",
    "            X_test_interpolated, y_test_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated = pd.DataFrame(X_train_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_train_interpolated.shape[1])])\n",
    "            X_train_interpolated[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated = pd.DataFrame(X_valid_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_valid_interpolated.shape[1])])\n",
    "            X_valid_interpolated[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated = pd.DataFrame(X_test_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_test_interpolated.shape[1])])\n",
    "            X_test_interpolated[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_pred_features:\n",
    "            X_train_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated_pred  = pd.DataFrame(X_train_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_train_interpolated_pred.shape[1])])\n",
    "            X_train_interpolated_pred [\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated_pred = pd.DataFrame(X_valid_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_valid_interpolated_pred.shape[1])])\n",
    "            X_valid_interpolated_pred[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated_pred = pd.DataFrame(X_test_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_test_interpolated_pred.shape[1])])\n",
    "            X_test_interpolated_pred[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_features:\n",
    "            X_train_flo= utils.get_flo_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo= utils.get_flo_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo= utils.get_flo_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo = pd.DataFrame(X_train_flo,columns=[f\"{i}_flo\" for i in range(X_train_flo.shape[1])])\n",
    "            X_train_flo[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo= pd.DataFrame(X_valid_flo,columns=[f\"{i}_flo\" for i in range(X_valid_flo.shape[1])])\n",
    "            X_valid_flo[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo= pd.DataFrame(X_test_flo,columns=[f\"{i}_flo\" for i in range(X_test_flo.shape[1])])\n",
    "            X_test_flo[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lat_features:\n",
    "            X_train_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lat = pd.DataFrame(X_train_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_train_flo_lat.shape[1])])\n",
    "            X_train_flo_lat[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lat= pd.DataFrame(X_valid_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_valid_flo_lat.shape[1])])\n",
    "            X_valid_flo_lat[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lat= pd.DataFrame(X_test_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_test_flo_lat.shape[1])])\n",
    "            X_test_flo_lat[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lon_features:\n",
    "            X_train_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lon = pd.DataFrame(X_train_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_train_flo_lon.shape[1])])\n",
    "            X_train_flo_lon[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lon= pd.DataFrame(X_valid_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_valid_flo_lon.shape[1])])\n",
    "            X_valid_flo_lon[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lon= pd.DataFrame(X_test_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_test_flo_lon.shape[1])])\n",
    "            X_test_flo_lon[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_unique_features:\n",
    "            X_train_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,train_dates,unique_id)\n",
    "            X_valid_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,valid_dates,unique_id)\n",
    "            X_test_flo_unique = utils.get_unique_pred_interpolated(cfg.flo_unique_dir,test_dates,unique_id)\n",
    "            \n",
    "            X_train_flo_unique = pd.DataFrame(X_train_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_train_flo_unique.shape[1])])\n",
    "            X_train_flo_unique[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_unique= pd.DataFrame(X_valid_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_valid_flo_unique.shape[1])])\n",
    "            X_valid_flo_unique[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_unique= pd.DataFrame(X_test_flo_unique,columns=[f\"{i}_flo_unique\" for i in range(X_test_flo_unique.shape[1])])\n",
    "            X_test_flo_unique[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_unique,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "\n",
    "        X_train.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_valid.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_test.drop(\"datetime\",axis=1,inplace=True)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = utils.train_lgbm(X_train, y_train, X_valid, y_valid, cfg.lgb_params)\n",
    "        save_path = OUTPUT_DIR + f\"/lgbm_fold{fold}.txt\"\n",
    "        model.save_model(save_path)\n",
    "\n",
    "        # Evaluate model\n",
    "        valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        mse = utils.compute_mse(y_valid, valid_preds)\n",
    "        mae = utils.compute_mae(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold + 1} MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        df.loc[df.datetime.isin(valid_dates),\"pred\"] = valid_preds\n",
    "        df.loc[df.datetime.isin(test_dates),\"pred\"] += test_preds\n",
    "\n",
    "    df.loc[df.datetime.isin(test_dates),\"pred\"] /= len(train_date_list_split)\n",
    "    df.loc[df.datetime.isin(train_date_list+test_dates),cfg.saved_cols].to_csv(OUTPUT_DIR+\"oof.csv\",index=False)\n",
    "\n",
    "    oof_mse = utils.compute_mse(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mse = utils.compute_mse(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    oof_mae = utils.compute_mae(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mae = utils.compute_mae(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    print('-'*40)\n",
    "    print(f\"Overall Out-of-Fold RMSE: {np.sqrt(oof_mse):.4f}\")\n",
    "    print(f\"Overall Out-of-Fold MAE: {oof_mae:.4f}\")\n",
    "    print()\n",
    "    print(f\"Overall Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "    print(f\"Overall Test MAE: {test_mae:.4f}\")\n",
    "    print('-'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75257\n",
      "[LightGBM] [Info] Number of data points in the train set: 6777, number of used features: 1078\n",
      "[LightGBM] [Info] Using GPU Device: Quadro M5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1078 dense feature groups (6.98 MB) transferred to GPU in 0.067552 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.213732\n",
      "[LightGBM] [Debug] Re-bagging, using 5379 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[1]\tvalid_0's l1: 0.333745\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[2]\tvalid_0's l1: 0.325145\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[3]\tvalid_0's l1: 0.317154\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[4]\tvalid_0's l1: 0.31003\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[5]\tvalid_0's l1: 0.303621\n",
      "[LightGBM] [Debug] Re-bagging, using 5354 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[6]\tvalid_0's l1: 0.296718\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[7]\tvalid_0's l1: 0.290744\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[8]\tvalid_0's l1: 0.284578\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[9]\tvalid_0's l1: 0.278438\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[10]\tvalid_0's l1: 0.272713\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's l1: 0.272713\n",
      "Fold 1 MSE: 0.10235367750997483, MAE: 0.27271280861791236\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 75046\n",
      "[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 1078\n",
      "[LightGBM] [Info] Using GPU Device: Quadro M5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1078 dense feature groups (7.02 MB) transferred to GPU in 0.067387 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.493143\n",
      "[LightGBM] [Debug] Re-bagging, using 5410 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[1]\tvalid_0's l1: 0.317572\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[2]\tvalid_0's l1: 0.310422\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[3]\tvalid_0's l1: 0.304752\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[4]\tvalid_0's l1: 0.299184\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[5]\tvalid_0's l1: 0.294227\n",
      "[LightGBM] [Debug] Re-bagging, using 5388 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[6]\tvalid_0's l1: 0.28966\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[7]\tvalid_0's l1: 0.285542\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[8]\tvalid_0's l1: 0.280597\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[9]\tvalid_0's l1: 0.276536\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[10]\tvalid_0's l1: 0.27251\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tvalid_0's l1: 0.27251\n",
      "Fold 2 MSE: 0.0879344960856595, MAE: 0.27250989494564415\n",
      "----------------------------------------\n",
      "Overall Out-of-Fold RMSE: 0.3085\n",
      "Overall Out-of-Fold MAE: 0.2726\n",
      "\n",
      "Overall Test RMSE: 0.1936\n",
      "Overall Test MAE: 0.1880\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "main(CFG6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

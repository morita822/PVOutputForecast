{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM001　ベースラインモデル\n",
    "- オプティカルフローの予測値を使いたい。\n",
    "- フローの速さを用いたい。\n",
    "- 過去の発電量をid指定で用いたい。\n",
    "- Interpolated_Dataset(Dataset)などをutils.pyにまとめたい。\n",
    "\n",
    "- observed_max2を修正する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 23909\n",
      "[LightGBM] [Info] Number of data points in the train set: 2221, number of used features: 973\n",
      "[LightGBM] [Info] Using GPU Device: Quadro M5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 973 dense feature groups (2.07 MB) transferred to GPU in 0.063127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.479855\n",
      "[LightGBM] [Debug] Re-bagging, using 1770 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[1]\tvalid_0's l1: 0.251699\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[2]\tvalid_0's l1: 0.242342\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2]\tvalid_0's l1: 0.242342\n",
      "Fold 1 MSE: 0.07533364912157169, MAE: 0.24234205099963235\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "c:\\Users\\Yosui\\.conda\\envs\\lab\\Lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 23903\n",
      "[LightGBM] [Info] Number of data points in the train set: 2231, number of used features: 973\n",
      "[LightGBM] [Info] Using GPU Device: Quadro M5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 973 dense feature groups (2.08 MB) transferred to GPU in 0.076377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.480733\n",
      "[LightGBM] [Debug] Re-bagging, using 1778 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[1]\tvalid_0's l1: 0.254274\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[2]\tvalid_0's l1: 0.24512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2]\tvalid_0's l1: 0.24512\n",
      "Fold 2 MSE: 0.07658722731495358, MAE: 0.24512020202806398\n",
      "----------------------------------------\n",
      "Overall Out-of-Fold RMSE: 0.2756\n",
      "Overall Out-of-Fold MAE: 0.2437\n",
      "\n",
      "Overall Test RMSE: 0.2602\n",
      "Overall Test MAE: 0.2256\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "import utils\n",
    "\n",
    "class CFG:\n",
    "    DEBUG = True\n",
    "    note_num = \"StackingOpt/LGBM003\"\n",
    "    n_splits = 2  #データの分割\n",
    "    seed = 42\n",
    "    \n",
    "    #model\n",
    "    lgb_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mae',\n",
    "                'num_boost_round':2,\n",
    "                'early_stopping_rounds':100,\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': 5,\n",
    "                'device_type': 'gpu',\n",
    "                'seed':42\n",
    "                }\n",
    "    \n",
    "    #日付\n",
    "    # train_start_date = \"201406010000\"\n",
    "    # train_end_date = \"201407010000\"\n",
    "    # test_start_date = \"201407010000\"\n",
    "    # test_end_date = \"201408010000\"\n",
    "\n",
    "    train_start_date = \"201308150000\"\n",
    "    train_end_date = \"20130817000\"\n",
    "    test_start_date = \"201308170000\"\n",
    "    test_end_date = \"20130818000\"\n",
    "\n",
    "    #特徴量\n",
    "    interpolated_dir = \"H:\\study\\output\\StackingOpt\\EDA005\"\n",
    "    pred_dir = \"H:\\study\\output\\StackingOpt\\EDA006\"\n",
    "    flo_dir = 'H:\\study\\output\\StackingOpt\\EDA006'\n",
    "\n",
    "    features = ['two_weeks_max', 'id', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'year_sin', 'year_cos',\\\n",
    "                'prev_30m_generation', 'id_lat', 'id_lng', 'id_lat_mesh', 'id_lng_mesh', 'pvrate', 'observed_max2']\n",
    "\n",
    "    use_interpolated_features = True\n",
    "    use_pred_features = True\n",
    "    use_flo_features = True\n",
    "    use_flo_lat_features = True\n",
    "    use_flo_lon_features = True\n",
    "\n",
    "\n",
    "    target = 'nv2'\n",
    "\n",
    "    #oofで保存するcol\n",
    "    saved_cols = [\"datetime\",\"id\",\"fold\",\"observed_max2\",\"generation\",target,\"pred\"]\n",
    "\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "\n",
    "    if cfg.DEBUG:\n",
    "        OUTPUT_DIR = f'H:/study/output/DEBUG/{cfg.note_num}/'\n",
    "    else:\n",
    "        OUTPUT_DIR = f'H:/study/output/{cfg.note_num}/'\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    utils.set_seeds()\n",
    "    # 時系列の分割設定\n",
    "    train_date_list = utils.create_time_series_data(cfg.train_start_date,cfg.train_end_date)\n",
    "    train_date_list_split = np.array_split(train_date_list, cfg.n_splits)\n",
    "\n",
    "    test_dates = utils.create_time_series_data(cfg.test_start_date,cfg.test_end_date)\n",
    "\n",
    "    #データセットの読み込み\n",
    "    unique_id = ['10000095', '10000269', '1020000002', '1110000001', '1110000010', '1110000011', '1110000012', '1110000013', '1110000014', '1110000015', '1160000025', '1160000090', '1160000091', '1160000182', '1160000185', '1160000253', '1160000387', '1160000402', '1160000419', '1160000420', '1160000423', '1270000026', '1280000048', '1550000001', '1650000004', '1680000001', '1680000002', '1680000003', '1680000004', '1680000010', '1680000017', '1680000021', '1680000033', '1680000047', '1680000054', '1680000057', '1680000063', '1680000067', '1680000080', '1680000081', '1680000097', '1680000107', '1680000108', '1680000112', '1680000151', '1680000152', '1680000213', '1680000216', '1680000217', '1680000218', '1680000223', '1680000228', '1680000285', '1680000287', '1680000327', '1680000364', '2220000001', '2220000002', '2220000003', '2730000001', '2910000002', '3000000007', '3000000012', '3000000042', '5000000044', '5000000045', '6000000016', '6000000017', '6060000016', '6060000017', '6060000018', '6170000016', '6170000123', '6170000124', '6170000125', '6620000065', '6620000066', '6620000088', '6620000089', '6620000111', '6620000117', '6620000118', '6620000121', '6620000122', '6620000123', '6620000124', '6620000131', '6620000132', '6910000180', '6910000198', '6910000200', '6910000206', '6910000216', '6910000217', '6910000239', '6910000240', '6910000249', '6910000250', '6910000421', '6910000424', '6910000425', '6910000469', '6910000470']\n",
    "    unique_id = [int(i) for i in unique_id]\n",
    "    to_unique_id = [str(num).zfill(10) for num in unique_id]\n",
    "    df = utils.get_preprocessing_data(to_unique_id)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.drop_duplicates(subset=[\"id\",\"datetime\"],inplace=True)\n",
    "    df = df.groupby('id').apply(utils.prev_30m_generation).reset_index(level=0, drop=True)\n",
    "    id_all_data = pd.read_csv(\"H:\\study\\output\\StackingOpt\\EDA006\\id_all_data.csv\")\n",
    "    df = df.merge(id_all_data,on=[\"id\"],how=\"left\")\n",
    "    df.dropna(subset=[\"year\"],inplace=True) #utils.prev_30m_generationで30分間隔のデータセットになっているため欠損が出ている。\n",
    "    df[\"nv2\"] = df[\"generation\"] / df[\"observed_max2\"]\n",
    "\n",
    "    #oof作成用\n",
    "    df[\"pred\"] = 0\n",
    "    df.loc[df.datetime.isin(test_dates),\"fold\"] = \"test\"\n",
    "    \n",
    "    for fold in range(len(train_date_list_split)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        train_dates = np.concatenate(train_date_list_split[:fold] + train_date_list_split[fold+1:])\n",
    "        valid_dates = train_date_list_split[fold]\n",
    "\n",
    "        X_train, y_train = df.loc[df.datetime.isin(train_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(train_dates),cfg.target]\n",
    "        X_valid, y_valid = df.loc[df.datetime.isin(valid_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(valid_dates),cfg.target]\n",
    "        X_test, y_test = df.loc[df.datetime.isin(test_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(test_dates),cfg.target]\n",
    "        df.loc[df.datetime.isin(valid_dates),\"fold\"] = fold\n",
    "\n",
    "        if cfg.use_interpolated_features:\n",
    "            X_train_interpolated, y_train_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,train_dates)\n",
    "            X_valid_interpolated, y_valid_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,valid_dates)\n",
    "            X_test_interpolated, y_test_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated = pd.DataFrame(X_train_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_train_interpolated.shape[1])])\n",
    "            X_train_interpolated[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated = pd.DataFrame(X_valid_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_valid_interpolated.shape[1])])\n",
    "            X_valid_interpolated[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated = pd.DataFrame(X_test_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_test_interpolated.shape[1])])\n",
    "            X_test_interpolated[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_pred_features:\n",
    "            X_train_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated_pred  = pd.DataFrame(X_train_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_train_interpolated_pred.shape[1])])\n",
    "            X_train_interpolated_pred [\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated_pred = pd.DataFrame(X_valid_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_valid_interpolated_pred.shape[1])])\n",
    "            X_valid_interpolated_pred[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated_pred = pd.DataFrame(X_test_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_test_interpolated_pred.shape[1])])\n",
    "            X_test_interpolated_pred[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_features:\n",
    "            X_train_flo= utils.get_flo_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo= utils.get_flo_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo= utils.get_flo_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo = pd.DataFrame(X_train_flo,columns=[f\"{i}_flo\" for i in range(X_train_flo.shape[1])])\n",
    "            X_train_flo[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo= pd.DataFrame(X_valid_flo,columns=[f\"{i}_flo\" for i in range(X_valid_flo.shape[1])])\n",
    "            X_valid_flo[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo= pd.DataFrame(X_test_flo,columns=[f\"{i}_flo\" for i in range(X_test_flo.shape[1])])\n",
    "            X_test_flo[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lat_features:\n",
    "            X_train_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lat = pd.DataFrame(X_train_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_train_flo_lat.shape[1])])\n",
    "            X_train_flo_lat[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lat= pd.DataFrame(X_valid_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_valid_flo_lat.shape[1])])\n",
    "            X_valid_flo_lat[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lat= pd.DataFrame(X_test_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_test_flo_lat.shape[1])])\n",
    "            X_test_flo_lat[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lon_features:\n",
    "            X_train_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lon = pd.DataFrame(X_train_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_train_flo_lon.shape[1])])\n",
    "            X_train_flo_lon[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lon= pd.DataFrame(X_valid_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_valid_flo_lon.shape[1])])\n",
    "            X_valid_flo_lon[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lon= pd.DataFrame(X_test_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_test_flo_lon.shape[1])])\n",
    "            X_test_flo_lon[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "\n",
    "        X_train.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_valid.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_test.drop(\"datetime\",axis=1,inplace=True)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = utils.train_lgbm(X_train, y_train, X_valid, y_valid, cfg.lgb_params)\n",
    "        save_path = OUTPUT_DIR + f\"/lgbm_fold{fold}.txt\"\n",
    "        model.save_model(save_path)\n",
    "\n",
    "        # Evaluate model\n",
    "        valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        mse = utils.compute_mse(y_valid, valid_preds)\n",
    "        mae = utils.compute_mae(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold + 1} MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        df.loc[df.datetime.isin(valid_dates),\"pred\"] = valid_preds\n",
    "        df.loc[df.datetime.isin(test_dates),\"pred\"] += test_preds\n",
    "\n",
    "\n",
    "    df.loc[df.datetime.isin(test_dates),\"pred\"] /= len(train_date_list_split)\n",
    "    df.loc[df.datetime.isin(train_date_list+test_dates),cfg.saved_cols].to_csv(OUTPUT_DIR+\"oof.csv\",index=False)\n",
    "\n",
    "    oof_mse = utils.compute_mse(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mse = utils.compute_mse(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    oof_mae = utils.compute_mae(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mae = utils.compute_mae(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    print('-'*40)\n",
    "    print(f\"Overall Out-of-Fold RMSE: {np.sqrt(oof_mse):.4f}\")\n",
    "    print(f\"Overall Out-of-Fold MAE: {oof_mae:.4f}\")\n",
    "    print()\n",
    "    print(f\"Overall Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "    print(f\"Overall Test MAE: {test_mae:.4f}\")\n",
    "    print('-'*40)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(CFG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM003.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "import utils\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "def main(cfg):\n",
    "\n",
    "    if cfg.DEBUG:\n",
    "        OUTPUT_DIR = f'H:/study/output/DEBUG/{cfg.note_num}/'\n",
    "    else:\n",
    "        OUTPUT_DIR = f'H:/study/output/{cfg.note_num}/'\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    utils.set_seeds()\n",
    "    # 時系列の分割設定\n",
    "    train_date_list = utils.create_time_series_data(cfg.train_start_date,cfg.train_end_date)\n",
    "    train_date_list_split = np.array_split(train_date_list, cfg.n_splits)\n",
    "\n",
    "    test_dates = utils.create_time_series_data(cfg.test_start_date,cfg.test_end_date)\n",
    "\n",
    "    #データセットの読み込み\n",
    "    unique_id = ['10000095', '10000269', '1020000002', '1110000001', '1110000010', '1110000011', '1110000012', '1110000013', '1110000014', '1110000015', '1160000025', '1160000090', '1160000091', '1160000182', '1160000185', '1160000253', '1160000387', '1160000402', '1160000419', '1160000420', '1160000423', '1270000026', '1280000048', '1550000001', '1650000004', '1680000001', '1680000002', '1680000003', '1680000004', '1680000010', '1680000017', '1680000021', '1680000033', '1680000047', '1680000054', '1680000057', '1680000063', '1680000067', '1680000080', '1680000081', '1680000097', '1680000107', '1680000108', '1680000112', '1680000151', '1680000152', '1680000213', '1680000216', '1680000217', '1680000218', '1680000223', '1680000228', '1680000285', '1680000287', '1680000327', '1680000364', '2220000001', '2220000002', '2220000003', '2730000001', '2910000002', '3000000007', '3000000012', '3000000042', '5000000044', '5000000045', '6000000016', '6000000017', '6060000016', '6060000017', '6060000018', '6170000016', '6170000123', '6170000124', '6170000125', '6620000065', '6620000066', '6620000088', '6620000089', '6620000111', '6620000117', '6620000118', '6620000121', '6620000122', '6620000123', '6620000124', '6620000131', '6620000132', '6910000180', '6910000198', '6910000200', '6910000206', '6910000216', '6910000217', '6910000239', '6910000240', '6910000249', '6910000250', '6910000421', '6910000424', '6910000425', '6910000469', '6910000470']\n",
    "    unique_id = [int(i) for i in unique_id]\n",
    "    to_unique_id = [str(num).zfill(10) for num in unique_id]\n",
    "    df = utils.get_preprocessing_data(to_unique_id)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.drop_duplicates(subset=[\"id\",\"datetime\"],inplace=True)\n",
    "    df = df.groupby('id').apply(utils.prev_30m_generation).reset_index(level=0, drop=True)\n",
    "    id_all_data = pd.read_csv(\"H:\\study\\output\\StackingOpt\\EDA006\\id_all_data.csv\")\n",
    "    df = df.merge(id_all_data,on=[\"id\"],how=\"left\")\n",
    "    df.dropna(subset=[\"year\"],inplace=True) #utils.prev_30m_generationで30分間隔のデータセットになっているため欠損が出ている。\n",
    "    df[\"nv2\"] = df[\"generation\"] / df[\"observed_max2\"]\n",
    "\n",
    "    #oof作成用\n",
    "    df[\"pred\"] = 0\n",
    "    df.loc[df.datetime.isin(test_dates),\"fold\"] = \"test\"\n",
    "    \n",
    "    for fold in range(len(train_date_list_split)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        train_dates = np.concatenate(train_date_list_split[:fold] + train_date_list_split[fold+1:])\n",
    "        valid_dates = train_date_list_split[fold]\n",
    "\n",
    "        X_train, y_train = df.loc[df.datetime.isin(train_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(train_dates),cfg.target]\n",
    "        X_valid, y_valid = df.loc[df.datetime.isin(valid_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(valid_dates),cfg.target]\n",
    "        X_test, y_test = df.loc[df.datetime.isin(test_dates),cfg.features+[\"datetime\"]],df.loc[df.datetime.isin(test_dates),cfg.target]\n",
    "        df.loc[df.datetime.isin(valid_dates),\"fold\"] = fold\n",
    "\n",
    "        if cfg.use_interpolated_features:\n",
    "            X_train_interpolated, y_train_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,train_dates)\n",
    "            X_valid_interpolated, y_valid_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,valid_dates)\n",
    "            X_test_interpolated, y_test_interpolated = utils.get_interpolated_mesh_data(cfg.interpolated_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated = pd.DataFrame(X_train_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_train_interpolated.shape[1])])\n",
    "            X_train_interpolated[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated = pd.DataFrame(X_valid_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_valid_interpolated.shape[1])])\n",
    "            X_valid_interpolated[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated = pd.DataFrame(X_test_interpolated,columns=[f\"{i}_interpolated\" for i in range(X_test_interpolated.shape[1])])\n",
    "            X_test_interpolated[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_pred_features:\n",
    "            X_train_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_interpolated_pred = utils.get_pred_interpolated_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_interpolated_pred  = pd.DataFrame(X_train_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_train_interpolated_pred.shape[1])])\n",
    "            X_train_interpolated_pred [\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_interpolated_pred = pd.DataFrame(X_valid_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_valid_interpolated_pred.shape[1])])\n",
    "            X_valid_interpolated_pred[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_interpolated_pred = pd.DataFrame(X_test_interpolated_pred,columns=[f\"{i}_pred\" for i in range(X_test_interpolated_pred.shape[1])])\n",
    "            X_test_interpolated_pred[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_interpolated_pred,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_features:\n",
    "            X_train_flo= utils.get_flo_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo= utils.get_flo_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo= utils.get_flo_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo = pd.DataFrame(X_train_flo,columns=[f\"{i}_flo\" for i in range(X_train_flo.shape[1])])\n",
    "            X_train_flo[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo= pd.DataFrame(X_valid_flo,columns=[f\"{i}_flo\" for i in range(X_valid_flo.shape[1])])\n",
    "            X_valid_flo[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo= pd.DataFrame(X_test_flo,columns=[f\"{i}_flo\" for i in range(X_test_flo.shape[1])])\n",
    "            X_test_flo[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lat_features:\n",
    "            X_train_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lat= utils.get_flo_lat_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lat = pd.DataFrame(X_train_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_train_flo_lat.shape[1])])\n",
    "            X_train_flo_lat[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lat= pd.DataFrame(X_valid_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_valid_flo_lat.shape[1])])\n",
    "            X_valid_flo_lat[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lat= pd.DataFrame(X_test_flo_lat,columns=[f\"{i}_flo_lat\" for i in range(X_test_flo_lat.shape[1])])\n",
    "            X_test_flo_lat[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lat,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "        if cfg.use_flo_lon_features:\n",
    "            X_train_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,train_dates)\n",
    "            X_valid_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,valid_dates)\n",
    "            X_test_flo_lon= utils.get_flo_lon_mesh_data(cfg.pred_dir,test_dates)\n",
    "            \n",
    "            X_train_flo_lon = pd.DataFrame(X_train_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_train_flo_lon.shape[1])])\n",
    "            X_train_flo_lon[\"datetime\"] = pd.to_datetime(train_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_valid_flo_lon= pd.DataFrame(X_valid_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_valid_flo_lon.shape[1])])\n",
    "            X_valid_flo_lon[\"datetime\"] = pd.to_datetime(valid_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_test_flo_lon= pd.DataFrame(X_test_flo_lon,columns=[f\"{i}_flo_lon\" for i in range(X_test_flo_lon.shape[1])])\n",
    "            X_test_flo_lon[\"datetime\"] = pd.to_datetime(test_dates, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "            X_train = X_train.merge(X_train_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_valid = X_valid.merge(X_valid_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "            X_test = X_test.merge(X_test_flo_lon,on=[\"datetime\"],how=\"left\")\n",
    "\n",
    "\n",
    "        X_train.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_valid.drop(\"datetime\",axis=1,inplace=True)\n",
    "        X_test.drop(\"datetime\",axis=1,inplace=True)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = utils.train_lgbm(X_train, y_train, X_valid, y_valid, cfg.lgb_params)\n",
    "        save_path = OUTPUT_DIR + f\"/lgbm_fold{fold}.txt\"\n",
    "        model.save_model(save_path)\n",
    "\n",
    "        # Evaluate model\n",
    "        valid_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        mse = utils.compute_mse(y_valid, valid_preds)\n",
    "        mae = utils.compute_mae(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold + 1} MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "        # Make predictions for the test set\n",
    "        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        df.loc[df.datetime.isin(valid_dates),\"pred\"] = valid_preds\n",
    "        df.loc[df.datetime.isin(test_dates),\"pred\"] += test_preds\n",
    "\n",
    "\n",
    "    df.loc[df.datetime.isin(test_dates),\"pred\"] /= len(train_date_list_split)\n",
    "    df.loc[df.datetime.isin(train_date_list+test_dates),cfg.saved_cols].to_csv(OUTPUT_DIR+\"oof.csv\",index=False)\n",
    "\n",
    "    oof_mse = utils.compute_mse(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mse = utils.compute_mse(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    oof_mae = utils.compute_mae(df.loc[df.datetime.isin(train_date_list),cfg.target] , df.loc[df.datetime.isin(train_date_list),\"pred\"])\n",
    "    test_mae = utils.compute_mae(df.loc[df.datetime.isin(test_dates),cfg.target] , df.loc[df.datetime.isin(test_dates),\"pred\"])\n",
    "\n",
    "    print('-'*40)\n",
    "    print(f\"Overall Out-of-Fold RMSE: {np.sqrt(oof_mse):.4f}\")\n",
    "    print(f\"Overall Out-of-Fold MAE: {oof_mae:.4f}\")\n",
    "    print()\n",
    "    print(f\"Overall Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "    print(f\"Overall Test MAE: {test_mae:.4f}\")\n",
    "    print('-'*40)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, default=\"CFG1\", help=\"設定クラスを選択（CFG1 または CFG2 など）\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config_module = importlib.import_module(\"LGBM003_config\")\n",
    "    cfg_classes = {name: cls for name, cls in inspect.getmembers(config_module, inspect.isclass) if name.startswith(\"CFG\")}\n",
    "\n",
    "    if args.config in cfg_classes:\n",
    "        cfg = cfg_classes[args.config]()\n",
    "    else:\n",
    "        raise ValueError(\"無効な設定クラス名が指定されました。\")\n",
    "\n",
    "    main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

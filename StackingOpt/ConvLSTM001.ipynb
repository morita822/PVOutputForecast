{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM_GPT4_001をもとにコードを書く\n",
    "- パラメーターの調整\n",
    "- バッチ間の処理を行いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch [20/600], Train Loss: 0.0333, Valid Loss: 0.0261\n",
      "Epoch [40/600], Train Loss: 0.0321, Valid Loss: 0.0247\n",
      "Epoch [60/600], Train Loss: 0.0308, Valid Loss: 0.0236\n",
      "Epoch [80/600], Train Loss: 0.0297, Valid Loss: 0.0226\n",
      "Epoch [100/600], Train Loss: 0.0292, Valid Loss: 0.0220\n",
      "Epoch [120/600], Train Loss: 0.0288, Valid Loss: 0.0217\n",
      "Epoch [140/600], Train Loss: 0.0287, Valid Loss: 0.0215\n",
      "Epoch [160/600], Train Loss: 0.0284, Valid Loss: 0.0214\n",
      "Epoch [180/600], Train Loss: 0.0283, Valid Loss: 0.0212\n",
      "Epoch [200/600], Train Loss: 0.0282, Valid Loss: 0.0211\n",
      "Epoch [220/600], Train Loss: 0.0281, Valid Loss: 0.0210\n",
      "Epoch [240/600], Train Loss: 0.0280, Valid Loss: 0.0210\n",
      "Epoch [260/600], Train Loss: 0.0280, Valid Loss: 0.0209\n",
      "Epoch [280/600], Train Loss: 0.0278, Valid Loss: 0.0208\n",
      "Epoch [300/600], Train Loss: 0.0280, Valid Loss: 0.0207\n",
      "Epoch [320/600], Train Loss: 0.0278, Valid Loss: 0.0206\n",
      "Epoch [340/600], Train Loss: 0.0279, Valid Loss: 0.0206\n",
      "Epoch [360/600], Train Loss: 0.0277, Valid Loss: 0.0205\n",
      "Epoch [380/600], Train Loss: 0.0276, Valid Loss: 0.0204\n",
      "Epoch [400/600], Train Loss: 0.0275, Valid Loss: 0.0204\n",
      "Epoch [420/600], Train Loss: 0.0274, Valid Loss: 0.0203\n",
      "Epoch [440/600], Train Loss: 0.0275, Valid Loss: 0.0203\n",
      "Epoch [460/600], Train Loss: 0.0274, Valid Loss: 0.0202\n",
      "Epoch [480/600], Train Loss: 0.0274, Valid Loss: 0.0201\n",
      "Epoch [500/600], Train Loss: 0.0274, Valid Loss: 0.0201\n",
      "Epoch [520/600], Train Loss: 0.0273, Valid Loss: 0.0200\n",
      "Epoch [540/600], Train Loss: 0.0273, Valid Loss: 0.0200\n",
      "Epoch [560/600], Train Loss: 0.0271, Valid Loss: 0.0199\n",
      "Epoch [580/600], Train Loss: 0.0270, Valid Loss: 0.0199\n",
      "Epoch [600/600], Train Loss: 0.0270, Valid Loss: 0.0198\n",
      "  Valid MSE for fold 0: 0.0198\n",
      "\n",
      "Fold 2\n",
      "Epoch [20/600], Train Loss: 0.0283, Valid Loss: 0.0430\n",
      "Epoch [40/600], Train Loss: 0.0260, Valid Loss: 0.0379\n",
      "Epoch [60/600], Train Loss: 0.0246, Valid Loss: 0.0356\n",
      "Epoch [80/600], Train Loss: 0.0237, Valid Loss: 0.0344\n",
      "Epoch [100/600], Train Loss: 0.0234, Valid Loss: 0.0342\n",
      "Epoch [120/600], Train Loss: 0.0233, Valid Loss: 0.0339\n",
      "Epoch [140/600], Train Loss: 0.0232, Valid Loss: 0.0335\n",
      "Epoch [160/600], Train Loss: 0.0233, Valid Loss: 0.0334\n",
      "Epoch [180/600], Train Loss: 0.0232, Valid Loss: 0.0334\n",
      "Early stopping after 20 epochs without improvement.\n",
      "  Valid MSE for fold 1: 0.0332\n",
      "\n",
      "Fold 3\n",
      "Epoch [20/600], Train Loss: 0.0325, Valid Loss: 0.0273\n",
      "Early stopping after 20 epochs without improvement.\n",
      "  Valid MSE for fold 2: 0.0281\n",
      "\n",
      "Overall Out-of-Fold MSE: 0.0270\n",
      "\n",
      "Overall Test MSE: 0.0276\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.loss\")\n",
    "\n",
    "class CFG:\n",
    "    make_folder = True\n",
    "    note_num = \"StackingOpt/ConvLSTM001\"\n",
    "    seed = 42\n",
    "\n",
    "\n",
    "OUTPUT_DIR = f'H:/study/output/{CFG.note_num}/'\n",
    "\n",
    "if CFG.make_folder:\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dates, input_dir, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.date_list = dates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.date_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_date = self.date_list[idx]\n",
    "        input_date = (datetime.strptime(self.date_list[idx], \"%Y%m%d%H%M\") - timedelta(minutes=30)).strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "        input_csv = os.path.join(self.input_dir, f\"interpolated_mesh_data_{input_date}.csv\")\n",
    "        target_csv = os.path.join(self.input_dir, f\"interpolated_mesh_data_{target_date}.csv\")\n",
    "\n",
    "        input_data = pd.read_csv(input_csv,index_col=0).values.reshape(1, 15, 12)\n",
    "        target_data = pd.read_csv(target_csv,index_col=0).values.reshape(1, 15, 12)\n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "            target_tensor = self.transform(target_tensor)\n",
    "\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "# 2次元ConvLSTMモデルは先程定義したものを使用\n",
    "class ConvLSTM2D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTM2D, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "\n",
    "        self.conv_lstm_cell = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=self.padding)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        h_cur, c_cur = hidden_state\n",
    "\n",
    "        combined = torch.cat((input_tensor, h_cur), 1)\n",
    "        gates = self.conv_lstm_cell(combined)\n",
    "\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        input_gate = torch.sigmoid(input_gate)\n",
    "        forget_gate = torch.sigmoid(forget_gate)\n",
    "        cell_gate = torch.tanh(cell_gate)\n",
    "        output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "        c_next = forget_gate * c_cur + input_gate * cell_gate\n",
    "        h_next = output_gate * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "def create_data_loaders(input_dir, batch_size, train_dates,valid_dates):\n",
    "    train_dataset = CustomDataset(train_dates, input_dir)\n",
    "    valid_dataset = CustomDataset(valid_dates, input_dir)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def time_series_split(start_date, end_date, input_dir, n_splits):\n",
    "    start_date = datetime.strptime(start_date, \"%Y%m%d%H%M\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y%m%d%H%M\")\n",
    "    delta = (end_date - start_date) // n_splits\n",
    "\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        train_start = start_date + delta * i\n",
    "        train_end = train_start + delta\n",
    "        valid_start = train_end\n",
    "        valid_end = valid_start + delta\n",
    "\n",
    "        splits.append(((train_start.strftime(\"%Y%m%d%H%M\"), train_end.strftime(\"%Y%m%d%H%M\")),\n",
    "                       (valid_start.strftime(\"%Y%m%d%H%M\"), valid_end.strftime(\"%Y%m%d%H%M\"))))\n",
    "\n",
    "    return splits\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, device, epochs, batch_size, hidden_channels, height, width, patience):\n",
    "    early_stop_counter = 0\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            current_batch_size = input_batch.size(0)\n",
    "            h, c = torch.zeros(current_batch_size, hidden_channels, height, width).to(device), torch.zeros(current_batch_size, hidden_channels, height, width).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h, c = model(input_batch, (h, c))\n",
    "\n",
    "            loss = criterion(h, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for input_batch, target_batch in valid_loader:\n",
    "                input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "                current_batch_size = input_batch.size(0)\n",
    "                h, c = torch.zeros(current_batch_size, hidden_channels, height, width).to(device), torch.zeros(current_batch_size, hidden_channels, height, width).to(device)\n",
    "\n",
    "                h, c = model(input_batch, (h, c))\n",
    "\n",
    "                loss = criterion(h, target_batch)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter > patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "def save_model(model, save_path, fold):\n",
    "    torch.save(model.state_dict(), save_path.format(fold))\n",
    "\n",
    "def load_model(model, load_path, device):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "def inference(model, test_loader, device, height, width, hidden_channels):\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_batch, _ in test_loader:\n",
    "            input_batch = input_batch.to(device)\n",
    "\n",
    "            current_batch_size = input_batch.size(0)\n",
    "            h, c = torch.zeros(current_batch_size, hidden_channels, height, width).to(device), torch.zeros(current_batch_size, hidden_channels, height, width).to(device)\n",
    "\n",
    "            h, c = model(input_batch, (h, c))\n",
    "\n",
    "            test_preds.append(h.cpu().numpy())\n",
    "\n",
    "    test_preds = np.concatenate(test_preds, axis=0)\n",
    "\n",
    "    return test_preds\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "def prepare_data_loaders(input_dir, batch_size, train_dates, valid_dates, test_dates):\n",
    "    train_loader, valid_loader = create_data_loaders(input_dir, batch_size, train_dates, valid_dates)\n",
    "    _, test_loader = create_data_loaders(input_dir, batch_size, test_dates, test_dates)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "def train_and_evaluate_fold(model, device, num_epochs, batch_size, hidden_channels, height, width, patience,\n",
    "                            input_dir, train_dates, valid_dates, test_dates, fold, criterion, optimizer,save_path):\n",
    "    train_loader, valid_loader, test_loader = prepare_data_loaders(input_dir, batch_size, train_dates, valid_dates, test_dates)\n",
    "\n",
    "    train(model, criterion, optimizer, train_loader, valid_loader, device, num_epochs, batch_size, hidden_channels, height, width, patience)\n",
    "\n",
    "    save_model(model, save_path, fold)\n",
    "    \n",
    "    valid_true = np.concatenate([target_batch.numpy() for _, target_batch in valid_loader], axis=0)\n",
    "    valid_preds = inference(model, valid_loader, device, height, width, hidden_channels)\n",
    "\n",
    "    valid_mse = compute_mse(valid_true, valid_preds)\n",
    "    print(f\"  Valid MSE for fold {fold}: {valid_mse:.4f}\")\n",
    "\n",
    "    test_preds = inference(model, test_loader, device, height, width, hidden_channels)\n",
    "    test_true = np.concatenate([target_batch.numpy() for _, target_batch in test_loader], axis=0)\n",
    "\n",
    "    return valid_true, valid_preds, test_true, test_preds\n",
    "\n",
    "def create_time_series_data(start_date, end_date):\n",
    "\n",
    "    start_date = datetime.strptime(start_date, \"%Y%m%d%H%M\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y%m%d%H%M\")\n",
    "    delta = timedelta(minutes=30)\n",
    "\n",
    "    data_list = []\n",
    "    date = start_date\n",
    "    while date <= end_date:\n",
    "        if date.hour > 6 and date.hour < 18:\n",
    "            data_list.append(date.strftime(\"%Y%m%d%H%M\"))\n",
    "        elif date.hour == 18 and date.minute == 0:\n",
    "            data_list.append(date.strftime(\"%Y%m%d%H%M\"))\n",
    "        date += delta\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ハイパーパラメータの設定\n",
    "    batch_size = 8\n",
    "    input_channels = 1\n",
    "    hidden_channels = 1\n",
    "    kernel_size = (3, 3)\n",
    "    height, width = 15, 12\n",
    "    num_epochs = 600\n",
    "    patience = 20\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    # 入力ディレクトリと日付の設定\n",
    "    input_dir = \"H:\\study\\output\\StackingOpt\\EDA004\"\n",
    "    train_start_date = \"201406010000\"\n",
    "    train_end_date = \"201407010000\"\n",
    "    test_start_date = \"201407010000\"\n",
    "    test_end_date = \"201407310000\"\n",
    "\n",
    "    # train_start_date = \"201308150000\"\n",
    "    # train_end_date = \"20130817000\"\n",
    "    # test_start_date = \"201308170000\"\n",
    "    # test_end_date = \"20130818000\"\n",
    "\n",
    "    # 時系列の分割設定\n",
    "    n_splits = 3\n",
    "    train_data_list = create_time_series_data(train_start_date,train_end_date)\n",
    "    train_date_list_split = np.array_split(train_data_list, n_splits)\n",
    "\n",
    "    test_dates = create_time_series_data(test_start_date,test_end_date)\n",
    "\n",
    "\n",
    "    # 保存先ディレクトリの設定\n",
    "    save_path = OUTPUT_DIR + \"/saved_model_fold_{}.pth\"\n",
    "    valid_preds_save_path = OUTPUT_DIR + \"/valid_preds_fold_{}.npy\"\n",
    "    test_preds_save_path = OUTPUT_DIR + \"/test_preds_fold_{}.npy\"\n",
    "\n",
    "    # 変数の初期化\n",
    "    oof_preds = None\n",
    "    oof_true = None\n",
    "    test_preds_ensemble = None\n",
    "\n",
    "    set_seeds()\n",
    "    for fold in range(len(train_date_list_split)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        train_dates = np.concatenate(train_date_list_split[:fold] + train_date_list_split[fold+1:])\n",
    "        valid_dates = train_date_list_split[fold] \n",
    "\n",
    "\n",
    "        model = ConvLSTM2D(input_channels, hidden_channels, kernel_size).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        valid_true, valid_preds, test_true, test_preds = train_and_evaluate_fold(\n",
    "            model, device, num_epochs, batch_size, hidden_channels, height, width, patience,\n",
    "            input_dir, train_dates, valid_dates, test_dates, fold, criterion, optimizer,save_path.format(fold + 1)\n",
    "        )\n",
    "\n",
    "        if oof_preds is None:\n",
    "            oof_preds = valid_preds\n",
    "            oof_true = valid_true\n",
    "            oof_dates = valid_dates\n",
    "            test_preds_ensemble = test_preds\n",
    "        else:\n",
    "            oof_preds = np.vstack([oof_preds, valid_preds])\n",
    "            oof_true = np.vstack([oof_true, valid_true])\n",
    "            oof_dates = np.hstack([oof_dates, valid_dates])\n",
    "            test_preds_ensemble += test_preds\n",
    "\n",
    "\n",
    "    test_preds_ensemble /= n_splits\n",
    "    oof_mse = compute_mse(oof_true, oof_preds)\n",
    "    test_mse = compute_mse(test_true, test_preds_ensemble)\n",
    "    print(f\"\\nOverall Out-of-Fold MSE: {oof_mse:.4f}\")\n",
    "    print(f\"\\nOverall Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "    np.save(f\"{OUTPUT_DIR}/oof_preds.npy\", oof_preds)\n",
    "    np.save(f\"{OUTPUT_DIR}/oof_true.npy\", oof_true)\n",
    "    np.save(f\"{OUTPUT_DIR}/test_preds_ensemble.npy\", test_preds_ensemble)\n",
    "    np.save(f\"{OUTPUT_DIR}/test_true.npy\", test_true)\n",
    "\n",
    "    np.save(f\"{OUTPUT_DIR}/oof_dates.npy\", oof_dates)\n",
    "    np.save(f\"{OUTPUT_DIR}/test_dates_ensemble\", test_dates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 出力確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dates = np.load(f\"{OUTPUT_DIR}/oof_dates.npy\")\n",
    "oof_true = np.load(f\"{OUTPUT_DIR}/oof_true.npy\")\n",
    "oof_preds = np.load(f\"{OUTPUT_DIR}/oof_preds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(len(oof_dates)):\n",
    "    mesh = pd.read_csv(f\"H:\\study\\output\\StackingOpt\\EDA004\\interpolated_mesh_data_{oof_dates[i]}.csv\",index_col=0)\n",
    "    t = pd.DataFrame(oof_true[i][0]).to_numpy().reshape(-1)-mesh.to_numpy().reshape(-1)\n",
    "    scores.append(t.sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.594444278944621e-07, 5.1169967774100655e-06, 9.692687486427369e-05)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(scores),np.max(scores),np.sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027037263"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_mse = compute_mse(oof_true, oof_preds)\n",
    "oof_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_ensemble = np.load(f\"{OUTPUT_DIR}/test_preds_ensemble.npy\")\n",
    "test_dates = np.load(f\"{OUTPUT_DIR}/test_dates_ensemble.npy\")\n",
    "\n",
    "test_true = np.load(f\"{OUTPUT_DIR}/test_true.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.594444278944621e-07 5.1169967774100655e-06 9.692687486427369e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.09545753117026082"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = []\n",
    "mesh_list = []\n",
    "for i in range(len(test_dates)):\n",
    "    mesh = pd.read_csv(f\"H:\\study\\output\\StackingOpt\\EDA004\\interpolated_mesh_data_{test_dates[i]}.csv\",index_col=0)\n",
    "    mesh = mesh.to_numpy().reshape(1, 15, 12)\n",
    "    test = test_true[i].reshape(1, 15, 12)\n",
    "    score.append(np.sum(mesh-test))\n",
    "    mesh_list.append(mesh)\n",
    "\n",
    "test_true_mesh = np.concatenate(mesh_list, axis=0)    \n",
    "\n",
    "print(np.min(scores),np.max(scores),np.sum(scores))\n",
    "\n",
    "test_mse = compute_mse(test_true, test_preds_ensemble)\n",
    "test_mse_mesh = compute_mse(test_true_mesh, test_preds_ensemble)\n",
    "test_mse-test_mse_mesh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_ensemble = np.load(f\"{OUTPUT_DIR}/test_preds_ensemble.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_ensemble[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストデータの整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_ensemble = np.load(f\"{OUTPUT_DIR}/test_preds_ensemble.npy\")\n",
    "test_dates = np.load(f\"{OUTPUT_DIR}/test_dates_ensemble.npy\")\n",
    "\n",
    "test_true = np.load(f\"{OUTPUT_DIR}/test_true.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = pd.read_csv(r\"H:\\study\\preprocessing_data\\3_mesh_place\\lati_zenkoku.csv\", header=None, index_col=None)\n",
    "lon = pd.read_csv(r\"H:\\study\\preprocessing_data\\3_mesh_place\\long_zenkoku.csv\", header=None, index_col=None)\n",
    "id_all_data = pd.read_csv(r\"H:\\study\\preprocessing_data\\id_all_data.csv\", encoding=\"shift_jis\")\n",
    "\n",
    "unique_id = ['10000095', '10000269', '1020000002', '1110000001', '1110000010', '1110000011', '1110000012', '1110000013', '1110000014', '1110000015', '1160000025', '1160000090', '1160000091', '1160000182', '1160000185', '1160000253', '1160000387', '1160000402', '1160000419', '1160000420', '1160000423', '1270000026', '1280000048', '1550000001', '1650000004', '1680000001', '1680000002', '1680000003', '1680000004', '1680000010', '1680000017', '1680000021', '1680000033', '1680000047', '1680000054', '1680000057', '1680000063', '1680000067', '1680000080', '1680000081', '1680000097', '1680000107', '1680000108', '1680000112', '1680000151', '1680000152', '1680000213', '1680000216', '1680000217', '1680000218', '1680000223', '1680000228', '1680000285', '1680000287', '1680000327', '1680000364', '2220000001', '2220000002', '2220000003', '2730000001', '2910000002', '3000000007', '3000000012', '3000000042', '5000000044', '5000000045', '6000000016', '6000000017', '6060000016', '6060000017', '6060000018', '6170000016', '6170000123', '6170000124', '6170000125', '6620000065', '6620000066', '6620000088', '6620000089', '6620000111', '6620000117', '6620000118', '6620000121', '6620000122', '6620000123', '6620000124', '6620000131', '6620000132', '6910000180', '6910000198', '6910000200', '6910000206', '6910000216', '6910000217', '6910000239', '6910000240', '6910000249', '6910000250', '6910000421', '6910000424', '6910000425', '6910000469', '6910000470']\n",
    "unique_id = [int(i) for i in unique_id]\n",
    "to_unique_id = [str(num).zfill(10) for num in unique_id]\n",
    "id_data = id_all_data[id_all_data.id.isin(unique_id)].reset_index(drop=True)\n",
    "\n",
    "min_lat,max_lat = id_data.id_lat.min(),id_data.id_lat.max()\n",
    "min_lng,max_lng = id_data.id_lng.min(),id_data.id_lng.max()\n",
    "userow = (lon.iloc[:,0]>=min_lng)&(lon.iloc[:,0]<=max_lng)\n",
    "usecol = (lat.iloc[0]>=min_lat)&(lat.iloc[0]<=max_lat)\n",
    "\n",
    "lat = lat.loc[userow,usecol]\n",
    "lon = lon.loc[userow,usecol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test_preds_ensemble.reshape(-1)\n",
    "dates_np = np.repeat(test_dates, lat.shape[0]*lat.shape[1])\n",
    "\n",
    "lat_np = lat.to_numpy().reshape(-1)\n",
    "lat_np_repeated = np.tile(lat_np, (len(test_dates)))\n",
    "\n",
    "lon_np = lon.to_numpy().reshape(-1)\n",
    "lon_np_repeated = np.tile(lon_np, (len(test_dates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(zip(dates_np,lat_np_repeated,lon_np_repeated,preds),columns=[\"datetime\",\"id_lat_mesh\",\"id_lng_mesh\",\"pred\"])\n",
    "preds[\"datetime\"] = pd.to_datetime(preds[\"datetime\"], format='%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>id_lat_mesh</th>\n",
       "      <th>id_lng_mesh</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>35.66</td>\n",
       "      <td>139.90</td>\n",
       "      <td>0.365956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>35.68</td>\n",
       "      <td>139.90</td>\n",
       "      <td>0.446136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>35.70</td>\n",
       "      <td>139.90</td>\n",
       "      <td>0.422442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>35.72</td>\n",
       "      <td>139.90</td>\n",
       "      <td>0.466838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-07-01 07:00:00</td>\n",
       "      <td>35.74</td>\n",
       "      <td>139.90</td>\n",
       "      <td>0.468135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124195</th>\n",
       "      <td>2014-07-30 18:00:00</td>\n",
       "      <td>35.80</td>\n",
       "      <td>140.18</td>\n",
       "      <td>0.693425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124196</th>\n",
       "      <td>2014-07-30 18:00:00</td>\n",
       "      <td>35.82</td>\n",
       "      <td>140.18</td>\n",
       "      <td>0.688104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124197</th>\n",
       "      <td>2014-07-30 18:00:00</td>\n",
       "      <td>35.84</td>\n",
       "      <td>140.18</td>\n",
       "      <td>0.676767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124198</th>\n",
       "      <td>2014-07-30 18:00:00</td>\n",
       "      <td>35.86</td>\n",
       "      <td>140.18</td>\n",
       "      <td>0.676586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124199</th>\n",
       "      <td>2014-07-30 18:00:00</td>\n",
       "      <td>35.88</td>\n",
       "      <td>140.18</td>\n",
       "      <td>0.614044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  id_lat_mesh  id_lng_mesh      pred\n",
       "0      2014-07-01 07:00:00        35.66       139.90  0.365956\n",
       "1      2014-07-01 07:00:00        35.68       139.90  0.446136\n",
       "2      2014-07-01 07:00:00        35.70       139.90  0.422442\n",
       "3      2014-07-01 07:00:00        35.72       139.90  0.466838\n",
       "4      2014-07-01 07:00:00        35.74       139.90  0.468135\n",
       "...                    ...          ...          ...       ...\n",
       "124195 2014-07-30 18:00:00        35.80       140.18  0.693425\n",
       "124196 2014-07-30 18:00:00        35.82       140.18  0.688104\n",
       "124197 2014-07-30 18:00:00        35.84       140.18  0.676767\n",
       "124198 2014-07-30 18:00:00        35.86       140.18  0.676586\n",
       "124199 2014-07-30 18:00:00        35.88       140.18  0.614044\n",
       "\n",
       "[124200 rows x 4 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "\n",
    "df = utils.get_preprocessing_data(to_unique_id)\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df = df.merge(id_data[[\"id\",\"id_lat_mesh\",\"id_lng_mesh\",\"pvrate\",\"observed_max\"]],on=[\"id\"],how=\"left\")\n",
    "\n",
    "date_range = pd.to_datetime(test_dates, format='%Y%m%d%H%M')\n",
    "df = df[df.datetime.isin(date_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(preds,on=[\"datetime\",\"id_lat_mesh\",\"id_lng_mesh\"],how=\"left\")\n",
    "\n",
    "df[\"pred*two_weeks_max\"] = df[\"pred\"]*df[\"two_weeks_max\"]\n",
    "df[\"nv*twoweeks_max\"] = df[\"nv\"]*df[\"two_weeks_max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"APE\"] = np.abs(df[\"pred*two_weeks_max\"]-df[\"nv*twoweeks_max\"])/df[\"observed_max\"]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.045869188632334"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"APE\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
